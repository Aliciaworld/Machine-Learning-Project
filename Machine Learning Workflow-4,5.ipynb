{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Machine Learning Workflow-4,5:\n",
    "\n",
    "The general machine learning projects will follow the following pipeline. However, the detailed implementation can vary. For example, oftentimes we will iterate some procedures, such as feature engineering and selection etc.\n",
    "1. Data cleaning and formatting\n",
    "2. Exploratory Data Analysis(EDA)\n",
    "3. Feature engineering and selection\n",
    "4. Split dataset and compare different models on a performance metric\n",
    "5. Perform hyperparameter tuning on the best model to optimize it \n",
    "6. Evaluate the best model on the testing set\n",
    "7. Interpret the model results\n",
    "8. Draw conclusions and write a well-documented report\n",
    "\n",
    "Most of the following explainations and codes are from [Will Koehrsen](https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%20Learning%20Project%20Part%202.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 24\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# Imputing missing values and scaling values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split dataset and compare different models \n",
    "For supervised machine learning, we should first check if all the target has values, if not we should remove them, then split data.\n",
    "- no_target_score = data[data[target].isna()]\n",
    "- target = data[data[target].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 split dataset\n",
    "#### separate out the features and target\n",
    "- features = target_score.drop(columns=target)\n",
    "- target = pd.DataFrame(data[target])\n",
    "\n",
    "#### replace the inf and -inf with nan\n",
    "- features = features.replace(np.inf: np.nan, -np.inf:np.nan)\n",
    "\n",
    "#### split into 70% training and 30% tresting \n",
    "x_train,x_test,y_train,y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Establish a baseline\n",
    "The goal to establish a baseline is to compare the model with the naive guess. if the model we built failed, it means the model may be not suited for this problem. This may be because we are not using the right models, or we need more data, or there is a simpler solution that does not require machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regression task, a good naive baseline is to predict the median value of the target on the training set for all examples on the test set. If the model can't do better than guessing the median value, we will need to rethink the approach. And we can also use mean absolute error as metric to evaluate the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to calculate mean absolute error\n",
    "def mean_abso_err(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the median guess and evaluate it on the test set.\n",
    "- baseline_guess = np.median(y_train)\n",
    "\n",
    "- print('The baseline guess is a score of %0.2f'% baseline_guess)\n",
    "- print('Baseline Performance on the test set: Mean Absolute Eroor = %0.4f'% mean_abso_err(y_test, baseline_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluating and Comparing Machine Learning Models\n",
    "In this section we will build, train, and evaluate several models. The objective is to determine which model holds the most promise for further hyperparameter tuning.\n",
    "\n",
    "we will compare models using the Mean Absolute Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Imputing Missing Values\n",
    "Standard machine learning models can't deal with missing values, which means we should fill in those missing values or discard the feautres with missing values. Since we already removed features with more than 50% missing values in the first part, herein we will focus on filling in those missing values, a process known as imputation. There are many ways to filling missing values, one of the relatively simple method is to replace missing values with the median of the column.\n",
    "\n",
    "\n",
    "In the code below, a sklearn imputer object will be created to fill in missing values with the median of the columns. Notice that the imputer will be trained using Imputer.fit method on the training data but not on the test data. But they will be transformed to training data and testing data by using Imputer.transform. This means the missing values in the testing set will be filled in with the median value of the corresponding columns in the training set. This is one way to avoid the problem known as data leakage where information from testing set 'leaks' into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an imputer object with a median filling strategy\n",
    "- imputer = SimpleImputer(stragegy = 'median')\n",
    "\n",
    "##### Train on the training features\n",
    "- imputer.fit(train_features)\n",
    "\n",
    "##### Transform both training and testing data\n",
    "- x_train = imputer.transform(tran_features)\n",
    "- x_test = imputer.transform(test_features)\n",
    "\n",
    "##### Check if the missing values is null\n",
    "- print('Missing values in training features: ', np.sum(np.isnan(x_train))\n",
    "- print('Missing values in testing features: ', np.sum(np.isnan(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Scaling Features\n",
    "Sometimes features are in different units, if we do not normalize them, some of important features will be easily ignored because they are too small compared with other large ones. ** Linear Regression and Random Forest do not requre feature scaling.* But other methods, such as support vector machines and k nearest neighbors, do require feature scaling because they take into account the Euclidean distance between observations.\n",
    "\n",
    "\n",
    "There are two ways to scale features:\n",
    "1. For each value, subtract the mean of the feature and divide by the standard deviation of the features. This is known as standardization and results in each feature having a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "\n",
    "2. For each value, subtract the minimum value of the feature and divide by the maximum minus the minimum for the feature(the range). This assures that all the values for a feature are between 0 and 1 and is called scaling to a range or normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the scaler object with a range of 0-1\n",
    "- scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "##### Fit on the training data\n",
    "- scaler.fit(X)\n",
    "\n",
    "##### Transform both the training and testing data\n",
    "- x_train = scaler.transform(x_train)\n",
    "- x_test = scaler.transform(x_test)\n",
    "\n",
    "##### Convert y to one-dimensional array（vector）??\n",
    "- y_train = np.array(train_labels).reshape((-1, ))\n",
    "- y_test = np.array(test_labels).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Models to Evaluate\n",
    "We will compare five different machine learning models:\n",
    "\n",
    "\n",
    "- Linear Regression\n",
    "\n",
    "\n",
    "- Support Vector Machine Regression\n",
    "\n",
    "\n",
    "- Random Forest Regression\n",
    "\n",
    "\n",
    "- Gradient Boosting Regression\n",
    "\n",
    "\n",
    "- K-Nearest Neighbors Regression\n",
    "\n",
    "\n",
    "Generally these models will perform decently, but should be optimized before actually using as a model. At first, we just want to determine the baseline performance of each model, then select the best performing model for further optimization using hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to calculate Mean Absolute Error\n",
    "def mean_abso_err(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "# Takes in a model, trains the model, and evaluate it on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # train the model\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    # Make predictions and evaluate\n",
    "    model_pred = model.predict(x_test)\n",
    "    model_mae = mean_abso_err(y_test, model_pred)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return model_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "- lr = LinearRegression()\n",
    "- lr_mae = fit_and_evaluate(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Regression\n",
    "- svm = SVR(C = 1000, gamma = 0.1)\n",
    "- svm_mae = fit_and_evaluate(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression\n",
    "- random_forest = RandomForestRegression(random_state = 60)\n",
    "- random_forest_mae = fit_and_evaluate(random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regression\n",
    "- gradient_boosted = GradientBoostingRegressor(random_state = 60)\n",
    "- gradient_boosted_mae = fit_and_evaluate(gradient_boosted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors Regression\n",
    "- knn = KNeighborsRegressor(n_neighbors = 10)\n",
    "- knn_mae = fit_and_evaluate(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression Performance on the test set: MAE = %0.4f' % lr_mae)\n",
    "print('Support Vector Machine Regression Performance on the test set: MAE = %0.4f' % svm_mae)\n",
    "print('Random Forest Regression Performance on the test set: MAE = %0.4f' % random_forest_mae)\n",
    "print('Gradient Boosted Regression Performance on the test set: MAE = %0.4f') % gradient_boosted_mae)\n",
    "print('K-Nearest Neighbors Regression Performance on the test set: MAE = %0.4f' % knn_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transfer abover maes to DataFrame\n",
    "- model_comparison = pd.DataFrame({'model':['Linear Regression','Support Vector Machine', 'Random Forest', 'Gradient Boosted', 'K-Nearest Neighbors'],'Mean Absolute Error':[lr_mae, svm_mae, random_forest_mae, gradient_boosted_mae, knn_mae]})\n",
    "\n",
    "##### Horizontal bar chart of test mae\n",
    "- model_comparison.sort_values('Mean Absolute Error', ascending = False).plot(x='model',y='Mean Absolute Error', kind = 'barh', edgecolor='black')\n",
    "\n",
    "- plt.xlabel('??')\n",
    "- plt.xticks(size=??)\n",
    "- plt.ylabel('??')\n",
    "- plt.yticks(size=??)\n",
    "- plt.title('??', size=20)\n",
    "\n",
    "We then compare the maes of different models with baseline we set in 4.2, if they are outperform the baseline, the model is applicable.(less than baseline mae??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform hyperparameter tuning on the best model to optimize\n",
    "What's the hyperparameters and model parameters:\n",
    "- **Hyperparameter** are best thought of as setting for a machine leraning algorithem that are tuned by the data scientist before training. Such as number of trees in the random forest, or the number of neighbors used in K Nearest Neighbors Regression.\n",
    "\n",
    "- Model **parameters** are what the model learns during training\n",
    "\n",
    "\n",
    "Tunning the model hyperparameters controls the balance of under vs over fitting in a model. A model that underfits has high bias, and occurs when our model does not have enough capacity(degress of freedom) to learn the relationship between the features and the target. A model that overfits has high variance and in effect has memorized the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Random Search and Cross Validation\n",
    "We can choose the best hyperparameters for a model through random search and cross validation\n",
    "- **Random search** refers to the method in which we choose hyperparameters to evaluate:we define a range of options, and **randomly select combinations to try**. This is in contrast to **grid search** which **evaluates every single combination specified**. Generally, random search is better when we have limited knowledge of the best model hyperparameters and we can use random search to narrow down the options and user grid search with a more limited range of opitions.\n",
    "\n",
    "\n",
    "- **Cross Validation** is the method used to **assess the performance of the hyperparameters**. Rather than splitting the dataset into training and validation sets which reduces the amount of training data, we use K-Fold Cross Validation. This means dividing the training data into K folds, and then going through an interative process where we first train on k-1 of the folds and then evaluate performance on the kth fold. We repeat this process K times so eventually we will have tested on every example in the training data with the key that each iteration we are testing on data that we did not train on. At the end of K-fold cross validation, we take the average error on each of the K iterations as the final performance and train the model on all the training dataset. \n",
    "\n",
    "\n",
    "When implement random search with cross validation to select the optimal hyperparameters for the model, let's say **Gradient Boosting Regressor**, we first define a grid, then perform an iterative process of: randomly sample a set of hyperparameters from the grid, evaluate the hyperparameters using 4-fold cross-validation, then select the parameters with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
