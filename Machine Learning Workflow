The general machine learning projects will follow the following pipeline. However, the detailed implementation can vary. For example, oftentimes we will iterate some procedures, such as feature engineering and selection etc.

Machine Learning Workflow:
1. Data cleaning and formatting
2. Exploratory Data Analysis(EDA)
3. Feature engineering and selection
4. Establish a baseline and compare different models on a performance metric
5. Perform hyperparameter tuning on the best model to optimize it 
6. Evaluate the best model on the testing set
7. Interpret the model results
8. Draw conclusions and write a well-documented report


1. Data cleaning and formatting
   1.1 Load in the Data and Examine 
       The defination of the target;
       Examine missing value: "Not Avaiable" or "np.nan". "Not Available" means the columns with numbers will not be represented as "numeric", and be "object" type instead ;
   
   1.2 Data Type 
       Check the all the column type by using 'data.info()'
  
   1.3 Convert Data to Correct Types
       Convert object-type columns with numbers and 'Not Available' into numeric datatype(float): "data.replace({'Not Available': np.nan})"
	   Convert object-type columns that contain numeric values into numeric datatype: for col in list(data.columns):
	                                                                                       # Select columns that should be numeric
											       if('##' in col or '##' in col):
                    	                                                                            data[col] = data[col].astype(float)
   
   1.4 Missing Values 
       Missing values are fine when we do Exploratory Data Analysis, but they have to be filled in for machine learning methods.
	   For analysize missing values, we usually calculate the number of missing values and the percentage of the total missing values for each column: 
	   def missing_values (df):
	   		# total missing values
			mis_val = df.isnull().sum()
			
			# percentage of missing values
			mis_val_percent = 100*df.isnull().sum() / len(df)
			
			# make a table with the result
			mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
			
			# rename the columns
			mis_val_table_ren_columns = mis_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})
			
			# sort the table by percentage of missing descending
			mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:, 1] !=0].sort_values(ascending=False).round(1)
			
			# print summary information
			print ('Your selected dataframe has ' + str(df.shape[1]) + ' columns.\n'
			       'There are ' + str(mis_val_table_ren_columns.shape[0] + ' columns that have missing values.')
		    
			# return the dataframe with missing information
			return mis_val_table_ren_columns
	    
		# get the columns with > n% missing
		missing_df = missing_values(data)
		missing_columns = list(missing_df[missing_df['% of Total Values']>50].index)		
		data = data.drop(columns = list(missing_columns))
		print('We will remove %d columns that have more than 50% missing values.' % len(missing_columns))																					  
																								  
2. Exploratory Data Analysis(EDA)
   EDA is an open-ended process where we make plots and calculate statistics in order to explore our data. The purpose is to find anomalies, patterns, trends, or relationships. 
   2.1 Single Variable Plots(Distribution of targets)
       such as 'plt.hist(data['##'].dropna())'
   
   2.2 Detect If Outlier and remove
       outliers are extreme values. One of its defination are any data values which lie more than 3 times the interquartile range below the first quartile or above the third quartile.
       On the low end, an extreme outlier is below 'First Quartile - 3* Interquartile Range'
       On the right end, an extreme outlier is above 'Third Quartile + 3* Interquartile Range'
       
       2.2.1 Find the outliers by above hist plot combined with the following:
       data['##'].describe()
       data['##'].dropna().sort_values().tail()
       
       2.2.2 Remove outlier
       # Calculate first and third quartile
       first_quartile = data['##'].describe()['25%']
       third_quartile = data['##'].describe()['75%']
       
       # Interquartile range
       iqr = third_quartile - first_quartile
       
       # remove outliers
       data = data[(data['##'] > (first_quartile -3*iqr) & (data['##'] < (thir_quartile + 3*iqr)]
       
    2.3 Look for Relationships (between categorical variables and target)
        For example:
	types = data.dropna(subset=['target']
	types = type['##'].value_counts()
	types = list(types[types.values > 100].index)
	
	for type in types:
		subset = data[data['##'] == type]
		sns.kdeplot(subset['target'].dropna(), label=type)
	
	plt.xlable('$$', size = 20)
	plt.ylabel('$$', size = 20)
	plt.title('$$', size= 28)
	## For any categorical variable, it have to be one-hot encoded before it is feed into a machine learning model.
    
	2.4 Correlations between Feature and Target
       This is to quantify correlations between the features and the target. It is a measure of the strength and direction of a linear relationship between two variables, in which a value of -1 means the two variables are perfectly negatively linearly correlated and a value of +1 means the two variables are perfectly positively linearly correlated. (Although there can be non-linear relationships between the feature and targets, this is still a good way to start exploring trends in data. We can use these values for selecting the features to employ in our model.
	    # Find all correlations and sort
		correlation_data = data.corr()['target'].sort_values()	
		
		#find the most negative correlations
		correlation_data.head(10)
		
		#find the most positive correlations
		correlations_data.tail(10)
		
		# to account for possible non-linear relationships, take square root and log transformations of the features and then calculate the correlation coefficients with the target. 
		# select the numeric columns
		numeric_subset = data.select_types('number')  
		# create columns with square root and log of numeric columns
		for col in numeric_subset.columns:
		      if col='target':
			      next
			  else:
			     numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])
				 numeric_subset['log_' + col] = np.log(numeric_subset[col])				 
		
		# try and capture any possible relationships between the categorical and the target, we will one-hot encode categorical columns.
		categorical_subset = data[['??','??','??']]
		# one hot encode
		categorical_subset = pd.get_dummies(categorical_subset)
		
		# Join the two dataframes together
		features = pd.concat([numeric_subset, categorical_subset], axis=1)
		
		# drop target without values
		features = features.dropna(subset=['score'])
		
		# find correlations with the target
		correlations = feature.corr()['target'].dropna().sort_values()
		
		#find the most negative correlations
		correlation_data.head(10)
		
		#find the most positive correlations
		correlations_data.tail(10)
		
		2.5 Two-variables Plots
		# use scatterplot to visualize the relationship between two variables. We can include additional variable using such as color of markers or size of markers. For example, we can plot two numeric variables against one another and use color to represent a third categorical variable.
		
		# use seaborn to plot a scatterplot of target and some numerical feature
		sns.lmplot('numerical feature', 'target', hue='categorical feature', data=??, scatter_kws={'alphs':0.8})
		
		2.6 Pairs Plot
		As a final exercise for EDA, we make a pairs plot between several different variables. This is a great way to examine many variables at the same time as it shows scatterplots between pairs of variables and histograms of single variables on the diagonal. For example, we can use the upper triangle to show the scatterplots, the diagonal show the histograms and the lower triangle show both the correlation coefficient between two variables and kernel density estimate of two variables.
		## extract the columns to plot
		plot_data = feature[["??","??","??"]]
		
		## replace the inf with nan
		plot_data = plot_data.replace({np.inf: np.nan, -np.inf:np.nan})
		
		#function to calculate correlation coefficient between two columns
		## kwargs allows you to pass keyworded variable length of arguments to a function. 
		def corr_func(x, y, **kwargs):
		    r = np.corrcoef(x,y)[0][1]
			ax=plt.gca()
			ax.annotate("r={:.2f}".format(r), xy=ï¼ˆ.2, .8), xycoords=ax.transAxes, size=20)
		# create the parigrid object
		grid = sns.PairGrid(data=plot_data)
		grid.map_upper(plt.scatter)
		grid.map_diag(plt.hist)
		grid.map_lower(corr_func)
		plt.suptitle('???', size=??)
		
3. Feature engineering and selection
After explored the tredns and relationships within the data, we can work on engineering a set of features for models. Before we go further, we should define what features are to be used for the model by feature engineering and selection.
Feature engineering and selection are iterative processes that weill require several attempts to get right. Oftetimes we will use the results of modeling, such as the feature importantces from a random forest, to go back and redo feature selection, or we might discover relationships that necessitate creating new varibles.
Feature engineering and selection are more important than the exact algorithm and hyperparameters used for the model. 
For feature engineering, we can select numerical variables and categorical variables, or add in the log transformation fo the numerical variables, and one-hot encode the categorical variables.
For feature selection , we should remove the collinear features
    # Feature Engineering
    # Before doing feature engineering and selection, usually cope the original data
	features = data.copy()  (?? confusing)
	
	#select the numeric columns
	numeric_subset = data.select_dtypes('number')
	
	#create columns with log of numeric columns
	for col in numeric_subset.columns:
	          if col='target'
			      next
			  else:
			       numeric_subset['log'+col] = np.log(numeric_subset[col]
	 
	 #one hot encode
	 categorical_subset = pd.get_dummies(data[["??","??"]]
	 
	 #join the two dataframes
	 features = pd.concat([numeric_subset, categorical_subset, axis=1)
	 
	 features.shape
     
	 # Feature Selection
	 Highly collinear features have a significant correlation coefficent between them. They are highly correlated because they are just slightly different means of calculating target. Removing collinear features is a method to reduce model complexity by decreasing the number of features and can help to increase model generalization.
	 # Find highly collinear features
	 plt.plot(x,y)
	 plt.title('???, R= %0.4f' % np.corrcoef(data[['??',"??']].dropna(), rowvar=False)[0][1])
	 
	 We will remove collinear features that have a correlation coefficient above a certain threshold with each other. For example, we can use a threshold of 0.6 which remove one of a pair of features if the correlation coefficient between the features exceeds this value.
	 This function is adapted from Stack Overflow answer(https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on/43104383#43104383)
	 ## function to remove the collinear features
	 
4. Establish a baseline and compare different models on a performance metric
5. Perform hyperparameter tuning on the best model to optimize it 
6. Evaluate the best model on the testing set

7. Interpret the model results
8. Draw conclusions and write a well-documented report
