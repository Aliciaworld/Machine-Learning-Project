{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Machine Learning Workflow-1,2:\n",
    "\n",
    "The general machine learning projects will follow the following pipeline. However, the detailed implementation can vary. For example, oftentimes we will iterate some procedures, such as feature engineering and selection etc.\n",
    "1. Data cleaning and formatting\n",
    "2. Exploratory Data Analysis(EDA)\n",
    "3. Feature engineering and selection\n",
    "4. Split dataset and compare different models on a performance metric\n",
    "5. Perform hyperparameter tuning on the best model\n",
    "6. Evaluate the model\n",
    "7. Interpret the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set default font size\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# imputing missing values and scaling values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Clearning and formatting\n",
    "### 1.1  check the data type\n",
    "For some dataset, the default index is range of numer, which means nothing, we can replay the previous index with the 'id' as following:\n",
    "- pd.read_csv('filename with path', index_col = colume_with_id)\n",
    "\n",
    "\n",
    "- data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert to correct types\n",
    "a) convert 'object' columns with numbers and 'Not Available'to np.nan:\n",
    "- data.replace({'Not Available': np.nan})\n",
    "\n",
    "b) convert 'object' columns to numeric: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    for col in list(data.columns):\n",
    "        if('??' in col or \"??\" in col):\n",
    "            data[col] = data[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) convert datetime to datetime\n",
    "- pd.to_datetime(data[datetime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Check missing values\n",
    "Missing values are fine when we do exploratory data analysis(EDA), but they have to be filled in Maching Learning models.\n",
    "- method 1:using mapping\n",
    "    - sns.heatmap(data.isnull(), yticklabels=False, cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- method 2: using defined functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def missing_values(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "   \n",
    "    mis_val_table = pd.concat([mis_val,mis_val_percent], axis=1)\n",
    "    mis_val_table = mis_val_table.rename(columns = {0:'Missing Values', 1: '% of Toal Values'})\n",
    "    \n",
    "    # sort the tabel by percentage of missing descending\n",
    "    mis_val_table = mis_val_table[mis_val_table.iloc[:, 1] !=0].sort_values(ascending = Flase).round(1)\n",
    "    \n",
    "    # print summary information\n",
    "    print('The data has' + str(df.shape[1]) + ' columns.\\n'\n",
    "        'There are ' + str(mis_val_table.shape[0] + ' columns that have missing values.')\n",
    "    \n",
    "    return mis_val_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get the columns with > 50 % missing values\n",
    "missing_data = missing_values(data)\n",
    "missing_columns = list(missing_data[missing_data['% of Total Values']>50].index)\n",
    "data = data.drop(columns = list(missing_columns))\n",
    "print('%d columns that have more than 50% missing values will be removed.'% len(missing_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Check outlier and remove:\n",
    "- method 1: using build-in function\n",
    "    - data.describe()\n",
    "    \n",
    "- method 2: using other function:\n",
    "\n",
    "Outlier are extreme values. One of its definationa are any data values which lie more than 3 times the interquartile range below the first quartile or above the thir quartile. Therefore, the lower end will be 'First Quartile - 3 * Interquartile Range', and the higher end will be 'Third Quartile + 3 * Interquartile Range'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first_quantile = data['???'].describe()['25%']\n",
    "third_quartile = data['???'].describe()['75%']\n",
    "\n",
    "# Interquartile range\n",
    "iqr = thir_quartile - first_quantile\n",
    "\n",
    "# remove outliers\n",
    "data = data[(data['???'] > (first_quartile - 3 * iqr)) & (data['???'] < (third_quartile - 3 * iqr))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "### 2.1 Single Variable Plots(distribution of targets)\n",
    "- plt.hist(data['target'].dropna())\n",
    "\n",
    "- data[target].hist(bins=?,edgecolor='black)  #This one is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Relationships between categorical variables and target\n",
    "In order to look at the effect of categorical variables on the target, we made density plots colored by the value of the categorical variables. In order not to clutter the plot, sometimes we prefer to limit the graph to variables that have more than n vlueas in the dataset.\n",
    "#### choose those types that has counts > n \n",
    "- types = data.dropna(subset = ['target']\n",
    "- types = types['??'].value_counts()\n",
    "- types = list(types[types.values > n].index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for type in types:\n",
    "    subset = data[data['??'] == type]\n",
    "    \n",
    "    # Density plot of target\n",
    "    sns.kdeplot(subset[target].dropna(), label = type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check the relationship between categorical variable and target, we can also use the barplot\n",
    "- sns.barplot(x=categorical varible, y=target, data=?, ax=ax[0 or 1])\n",
    "\n",
    "\n",
    "for ax[0 or 1] is for there are more than one figures and we need to locate which figure we want to put it in."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* For any categorical variables, it has to be one-hot encoded before it is fed into a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Correlation coefficients between all variables(features) and target\n",
    "This is to quantify correlations between the features and target.And we can obtain the strength and direction of a linear relationship between two variables.\n",
    "#### a) find the correlation coefficients\n",
    "- correlation_data = data.corr()[target].sort_values()\n",
    "\n",
    "\n",
    "- correlation_data.head(10)  (This is to find the most negative correlations)\n",
    "\n",
    "\n",
    "- correlation_data.tail(10)  (This is to find the most positive correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for possible non-linear relationship, we can take square root and log transformation of the varibalbes and then calculate the correlation coefficients with target.\n",
    "#### b) take square root and log transformation(numeric columns)\n",
    "- numeric_subset = data.select_types('number')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for col in numeric_subset.columns:\n",
    "    if col=target:\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])\n",
    "        numeric_subset['log_' + col] = np.log(numeric_subset[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) use one-hot encode to capture any possible relationships between the categorical and target (categorical columns)\n",
    "- categorical_subset = data[['??', '??', '??'...]]\n",
    "\n",
    "\n",
    "- categorical_subset = pd.get_dummies(categorical_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Join the above (b) and (c) together\n",
    "- new_features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "\n",
    "\n",
    "- new_feautres = new_features.dropna(subset=target) (This is to drop target without values)\n",
    "\n",
    "\n",
    "- new_correlations = new_features.corr()[target].dropna().sort_values()\n",
    "\n",
    "\n",
    "- new_correlations = new_correlations.head(10)\n",
    "\n",
    "\n",
    "- new_correlations = new_correlations.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Two-variable relationship plots(between numerical columns)\n",
    "#### a) for several numerical variables (more than 2)\n",
    "- plot_data = new_features[['??', '??', '??']] (This is to choose how many features need to plot)\n",
    "- plot_data = plot_data.replace({np.inf:np.nan, -np.inf:np.nan}) (This is to replace the inf with nan)\n",
    "- plot_data = plot_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate correlation coefficient between two columns\n",
    "def corr_coef(x,y,**kwargs):\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate('r = {:.2f}'.format(r),xy = (.2, .8), xycoords = ax.transAzes, size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grid = sns.PairGrid(data = plot_data, size =3)\n",
    "\n",
    "\n",
    "- grid.map_upper(plt.scatter)\n",
    "\n",
    "\n",
    "- grid.map_diag(plt.hist, edgecolor = 'black')\n",
    "\n",
    "\n",
    "- grid.map_lower(corr_coef)\n",
    "\n",
    "\n",
    "- grid.map_lower(sns.kdeplot)\n",
    "\n",
    "\n",
    "- plt.suptitle('???', size=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) for two numerical variables\n",
    "Use scatterplot to visualize the relationship between two variables. We can also include additional variable by using color, size of markers to represent a thir categorical variable.\n",
    "- sns.lmplot(numerical feature, target, hue = categorical feature, data = new_features, scatter_kws = {'alpha':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
