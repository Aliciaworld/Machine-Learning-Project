{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove all the features that will not help creat an effective XGBoost model\n",
    "\n",
    "\n",
    "2. Reformat the columns names(tree can't have name separated by ' ')\n",
    "\n",
    "\n",
    "3. Identify missing values(such as st to 0)\n",
    "                              \n",
    "\n",
    "4. Transfer \"object\" to \"numeric\"(to those that seemed to be numeric but dtype showed to be object: pd.to_numeric())\n",
    "\n",
    "\n",
    "5. Transfer **categorical** features to **numeric** features  \n",
    "\n",
    "\n",
    "6. For imbalanced data, using \"stratify = y\" to maintain the same percentage in bothin training and testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', \n",
    "                   missing = None, \n",
    "                   seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf_xgb .fit(x_train, \n",
    "         y_train, \n",
    "         verbose = True, \n",
    "         early_stopping_rounds = 10, \n",
    "         eval_metric = 'aucpr', \n",
    "         eval_set = [(x_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metric import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf_xgb,\n",
    "               x_test,\n",
    "               y_test,\n",
    "               values_format = 'd',\n",
    "               display_labels = ['??', '??'])\n",
    "\n",
    "# the last two parameters are to make the confusion_matrix pretty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GridSearchCV()**: there are many hyperparameters to optimize, use GridSearchCV to test all possible combinations of the parameters\n",
    "\n",
    "\n",
    "- **scale_pos_weight**: help with imbalanced data\n",
    "\n",
    "    \n",
    "- **gamma**: the parameter that encourages pruning\n",
    "\n",
    "\n",
    "- **reg_lambda**: the regulation parameter lambda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "param_grid = {'max_depth': [3, 4, 5],\n",
    "          'learning_rate': [0.1, 0.001, 0.05],\n",
    "          'gamma': [0, 0.25, 1.0],\n",
    "          'reg_lambda': [0, 1.0, 10.0],\n",
    "          'scale_pos_weight': [1, 3, 5]}\n",
    "# If result showed some parameters are at the end of their range, continue optimise."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To speed up cross-validation, to further prevent overfitting, we only use a random subset of the data(90%) and a random subset of the features (50%) per tree.\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                   seed = 42,\n",
    "                   subsample = 0.9,\n",
    "                   colsample_bytree = 0.5)\n",
    "\n",
    "optimal_params = GridSearchCV(estimator = clf_xgb,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'roc_auc',\n",
    "                    verbose = 0,\n",
    "                    n_jobs = 10,\n",
    "                    cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit again after hyperparameter tune"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimal_params.fit(x_train,\n",
    "             y_train,\n",
    "             verbose = True,\n",
    "             early_stopping_rounds = 10,\n",
    "             eval_metric = 'auc',\n",
    "             eval_set = [(x_test, y_test)]\n",
    "\n",
    "optimal_params.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using optimized parameters to build the fianl XGBoost model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic',\n",
    "                   seed = 42,\n",
    "                   subsample = 0.9,\n",
    "                   colsample_bytree = 0.5,\n",
    "                            \n",
    "                   max_depth = 4,\n",
    "                   learning_rate = 0.1,\n",
    "                   gamma = 0.25,\n",
    "                   reg_lambda = 10,\n",
    "                   scale_pos_weight = 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf_xgb.fit(x_train,\n",
    "        y_train,\n",
    "        verbose = True,\n",
    "        early_stopping_rounds = 10,\n",
    "        eval_metric = 'aucpr'\n",
    "        eval_set = [(x_test, y_test])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metric import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf_xgb,\n",
    "               x_test,\n",
    "               y_test,\n",
    "               values_format = 'd',\n",
    "               display_labels = ['??', '??'])\n",
    "\n",
    "# the last two parameters are to make the confusion_matrix pretty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If want to get information, like gain and cover etc, at each node in the first tree, we just build the first tree(n_estimator = 1), otherwise we'll get the average over all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
