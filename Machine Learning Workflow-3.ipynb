{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Machine Learning Workflow-3:\n",
    "\n",
    "The general machine learning projects will follow the following pipeline. However, the detailed implementation can vary. For example, oftentimes we will iterate some procedures, such as feature engineering and selection etc.\n",
    "1. Data cleaning and formatting\n",
    "2. Exploratory Data Analysis(EDA)\n",
    "3. Feature engineering and selection\n",
    "4. Establish a baseline and compare different models on a performance metric\n",
    "5. Perform hyperparameter tuning on the best model to optimize it \n",
    "6. Evaluate the best model on the testing set\n",
    "7. Interpret the model results\n",
    "8. Draw conclusions and write a well-documented report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Feature Selection\n",
    "Feautre engineering and feature selection are iterative processes that will usually require several attempts to get right. It is more important than the exact algorithem and hyperparameters used for the model.\n",
    "\n",
    "### 3.1 Feature Engineering\n",
    "According to the above EDA, such as one-hot encode of categorical variables, and square root and log transformation fo the numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection\n",
    "Highly collinear features have a significant correlation coefficient between them. And the reason why they are highly correlated is because they are just sligtly different means of calculating the target. We can visualize how related two variables are by the following codes\"\n",
    "- plot_data = data[['??','??']].dropna()\n",
    "- plt.plot(plot_data['??'], plot_data['??'])\n",
    "- plt.title('?? vs ??, r = %0.4f'% np.corrcoef(data[['??','??']].dropna())[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing collinear features is a method to reduce model complexity by decreasing the number of features and can help to increase model generalization. We can remove features that have a correlation coefficient above a certain threshold(let's say 0.6) with each other. The codes below are from [Stack Over Flow](https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on/43104383#43104383)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def remove_coolinear_features(x, threshold):\n",
    "    y = x[target]\n",
    "    x = x.drop(columns = [target])\n",
    "    \n",
    "    # calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) -1)\n",
    "    drop_cols = []\n",
    "    \n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1),(i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            if val >= threshold:\n",
    "                drop_cols.append(i)\n",
    "    \n",
    "    drops = sorted(set(drop_cols))[::-1]\n",
    "   \n",
    "    # drops the correlated columns\n",
    "    for i in drops:\n",
    "        col = x.iloc[:,(i+1):(i+2)].columns.values\n",
    "        df = x.drop(col, axis=1)\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- features = remove_collinear_features(new_features, 0.6) (This is to remove the collinear features above a specified correlation coefficient)\n",
    "\n",
    "\n",
    " \n",
    "- features = features.dropna(axis =1, how='all')  (This is to remove any columns with all na values)\n",
    "\n",
    "\n",
    "- features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some models, such as random forest, perform implicit feature selection and automatically determine which features are important during training. So, we can keep the features we achieved now and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
